{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_Adam_optimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Petrichoeur/Neural_Net_from_scratch/blob/master/NN_Adam_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERZ0nKzsJNX",
        "colab_type": "text"
      },
      "source": [
        "## Import "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTmwbnGGxKQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    import numpy as np \n",
        "    import pandas as pd  \n",
        "    from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtuMXKxWsMMu",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmIzvIFNxLzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_prime(x): \n",
        "    ''' Useful function for later \n",
        "     All derivative are kinda easy , but Relu derivative is not well fit for numpy wise simple calculation , \n",
        "     so i have to huse a pre_made function to make it easier ''' \n",
        "    tmp=x\n",
        "    tmp[x<=0] = 0\n",
        "    tmp[x>0] = 1\n",
        "    return tmp \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DenseLayer() : \n",
        "\n",
        "    def __init__(self,size,activation='tanh',out=False):  \n",
        "        \n",
        "        self.out= out # For make some prediction later\n",
        "        self.size = size   # Size of the layer\n",
        "        self.activation=activation # Type of activation\n",
        "        self.weights=None  # Weights\n",
        "        self.weights_shape=None # Input_shape of weights\n",
        "        self.delta=None   # Delta for backward pass\n",
        "        self.output=None  # Output activation for the current layer \n",
        "        self.bias=None   # Bias \n",
        "        self.derivative=None # Derivative of the output activation \n",
        "        self.momentum= [0,0]\n",
        "        self.momentum_bias= [0,0]\n",
        "      \n",
        "    def weight_init(self): \n",
        "        np.random.seed(0) # Seed so we always have the same initialisation, so we can compare different activation function and different optimizer\n",
        "        self.weights=2*np.random.rand(self.weights_shape,self.size)-1  # Init the weights beetween [-1,1]\n",
        "        self.bias=np.ones((1,self.size)) \n",
        "\n",
        "    def Output(self,input_):  \n",
        "        '''Input_ = Input_ from the previous layer for forward pass    \n",
        "           At each pass the output is calculated but the derivative of the output too  \n",
        "           It's a way of having always the specific activation and derivative function for each layer. \n",
        "             '''\n",
        "        self.function=self.activation  # Wich type of activation we will use \n",
        "        #self.output= 1/(1+np.exp(-(np.dot(input_,self.weights)+self.bias))).reshape(1,self.size) \n",
        "        self.dot_activation=np.dot(input_,self.weights)+self.bias # We get the input dot the weights with add of the bias \n",
        "\n",
        "        if self.function=='tanh' :    # Hyperbolique tangente \n",
        "            self.output=np.tanh(self.dot_activation)  \n",
        "            self.derivative= (1-self.output**2)\n",
        "        elif self.function =='sigmoid' : # Sigmoid\n",
        "            self.output = 1/(1+np.exp(-1.0*self.dot_activation))  \n",
        "            self.derivative = self.output*(1-self.output)\n",
        "        #elif self.function == 'Relu' :  # Rectified Linear Unit\n",
        "         #   self.output= np.maximum(self.dot_activation,0)  \n",
        "          #  self.derivative=relu_prime(self.output)    # Some issues with that one !\n",
        "        elif self.function == 'Lecun_tanh':   # Lecun hyperbolic tangente activation \n",
        "            self.output= 1.7159*np.tanh((2/3)*self.dot_activation)\n",
        "            self.derivative =1.7159*(2/3)*(1-((self.output / 1.7159)**2)) \n",
        "        elif self.function=='nothing': \n",
        "            self.output = self.dot_activation \n",
        "            self.derivative= 1  \n",
        "        else :  \n",
        "             raise ValueError('Activation function unknown')\n",
        "         \n",
        "\n",
        "\n",
        "class NeuralNet():   # Class for Neural Net , It initialize the neural net when call .\n",
        "    def __init__(self,input_shape,batch_size=1): \n",
        "        self.input_shape=input_shape # Shape of the input\n",
        "        self.batch_size = 1  #Batch size for further implementation of Mini Batch Descend and other optimizer than SGD\n",
        "        self.layers={}  # A dic of Layer, better than list for memory use \n",
        "        self.input_layer=np.empty((1,input_shape)) # Input_layer size .\n",
        "        self.nn_size=0   # Number of Layers, Input doesn't count.\n",
        "        self.layer_shape=[input_shape] # Shape of each layer \n",
        "        self.forward_count=0 # To keep the count , just in case \n",
        "        self.pred=None  # To get the output after a forward pass\n",
        "        self.output_error=None # The error to minimize  \n",
        "        self.backward_count=0 # Number of backward pass  \n",
        "\n",
        "    def add(self,layer): \n",
        "        self.layers[self.nn_size]=layer   # We add a layer\n",
        "        self.layers[self.nn_size].weights_shape=self.layer_shape[self.nn_size] # We initialize the shape to match other layers \n",
        "        self.layers[self.nn_size].weight_init() # We initialize the weights .\n",
        "        self.layer_shape.append(self.layers[self.nn_size].size) # We keep the size for next layer use .\n",
        "        self.nn_size += 1  # We are getting bigger !!!!\n",
        "    \n",
        "    def forward(self,X,return_=False): \n",
        "        self.forward_count += 1 # Keep the count ! \n",
        "        self.input_layer=X # First input layer is the data to use \n",
        "        self.layers=OrderedDict(self.layers.items()) # You don't have to, but it's a way to preventing some index mistakes.\n",
        "        for i in range(self.nn_size) :   # Loop for passing the information through the network \n",
        "            if i==0:\n",
        "                self.layers[i].Output(self.input_layer)  # We calculate the first output\n",
        "            else : \n",
        "                self.layers[i].Output(self.layers[i-1].output) # We calculate the output depending the previous output \n",
        "        self.pred=self.layers[self.nn_size-1].output[0]  # The last outputs, aka the prediction.\n",
        "        if return_ == True :  # If you want to keep only the prediction\n",
        "            return  self.pred \n",
        "    def adam_op(self,momentum,grad,timestep,beta1=0.9,beta2=0.99,eps=1e-8): # function to calculate the update-rule with adam optimisation\n",
        "        m1=momentum[0] # First order  momentum  || Moving average of the gradient \n",
        "        m2=momentum[1] # Second order momentum, || Uncentered variance of the gradient\n",
        "        m1_ = beta1*m1 + (1-beta1)*grad\t # Update  and calculation of the moving average depending the last iteration\n",
        "        m2_ = beta2*m2 + (1-beta2)*np.square(grad) # Update  and calculation of the uncentered variance depending the last iteration\n",
        "        m1_hat = m1_/(1-(beta1**timestep))\t# Bias correction for the estimation of the first order  momentum \n",
        "        m2_hat = m2_/(1-(beta2**timestep))\t# Bias correction for the estimation of the second order  momentum \t\t \n",
        "\n",
        "        return m1_,m2_,np.divide(m1_hat,np.sqrt(m2_hat)+eps)\n",
        "    def Backward(self,y,X_train,alpha=0.02,optimizer='SGD'): \n",
        "        ''' FOr this function i won't go into mathematics details because you can easily find all the \n",
        "        maths you need on backward pass , gradient descend and weights updates. All you have to remember is : \n",
        "         for each layer , the delta is :  \n",
        "                Layer_error =(delta of the next layer dot the weights of the actual layer ) \n",
        "              (Not exactly but i use this for  my algorithm)  Layer _ delta =   Layer_error *( the derivative of the output of the previous layer )  \n",
        "              ( It eventually do the same stuff as basic SGD, but i keep the bias of the previous layer in each layer , so i have to do it differently)\n",
        "                update_on_the_weights_of_the_actual_layer == The weights + learning_rate*Previous_layers_output*Next_layers_delta  \n",
        "                \n",
        "                '''\n",
        "        self.X_train = X_train # Crazy train on the station !!! The data we will use for backward pass\n",
        "        self.output_error= y-self.pred  # The error on prediction.\n",
        "        self.output_delta=self.output_error*self.layers[self.nn_size-1].derivative # The delta of the output !!\n",
        "        for i in range(self.nn_size-1,0,-1):  # Reversed range , we start from the end\n",
        "            if i==self.nn_size-1:\n",
        "                self.layers[i].error=self.output_delta.dot(self.layers[i].weights.T)  #First layer error is special, output_delta it is \n",
        "            else : \n",
        "                self.layers[i].error=self.layers[i+1].delta.dot(self.layers[i].weights.T) #others layers errors act the same , mainstream layers .\n",
        "            self.layers[i].delta=self.layers[i].error*self.layers[i-1].derivative  # We get the delta values for each layers\n",
        "        if optimizer == 'SGD':\n",
        "            for i in range(self.nn_size-1) :  \n",
        "                if i ==0 : \n",
        "                    self.layers[i].weights += alpha*self.X_train.T.dot(self.layers[i+1].delta) # Update the weights , special because it's the input\n",
        "                    self.layers[i].bias += alpha*self.layers[i+1].delta # Update the bias , special too\n",
        "                elif i ==self.nn_size-1 :  \n",
        "                    self.layers[i].weights +=alpha*self.layers[i-1].output.T.dot(self.output_delta) # Update the weights, special because \n",
        "                                                                                                                #it's the end of the backwardpass journey\n",
        "                    self.layers[i].bias += alpha*self.output_delta # Update the bias \n",
        "                else : \n",
        "                    self.layers[i].weights += alpha*self.layers[i-1].output.T.dot(self.layers[i+1].delta)  # Update the weights\n",
        "                    self.layers[i].bias += alpha*self.layers[i+1].delta # Update the bias  \n",
        "        if optimizer =='Adam':   \n",
        "            \n",
        "            for i in range(self.nn_size-1):  \n",
        "                m =self.layers[i].momentum  # We take the momentums from weights\n",
        "                m_b= self.layers[i].momentum_bias # We take the momentums from bias \n",
        "                timestep=self.backward_count # numbers of iterations done on the train data set .\n",
        "                \n",
        "                if i ==0 : \n",
        "\n",
        "                    m[0],m[1],ADAM= self.adam_op(m,self.X_train.T.dot(self.layers[i+1].delta),timestep) # Get the new momentum and the update function for weights\n",
        "                    m_b[0],m_b[1],ADAM_bias= self.adam_op(m_b,self.layers[i+1].delta,timestep) # Get the new momentum and the update function for bias \n",
        "                    self.layers[i].weights += alpha*ADAM# Update the weights , special because it's the input\n",
        "                    self.layers[i].bias += alpha*ADAM_bias # Update the bias , special too   \n",
        "\n",
        "                  \n",
        "                                \n",
        "                elif i ==self.nn_size-1 : \n",
        "                    m[0],m[1],ADAM= self.adam_op(m,self.layers[i-1].output.T.dot(self.output_delta),timestep) # Get the new momentum and the update function for weights\n",
        "                    m_b[0],m_b[1],ADAM_bias=self.adam_op(m_b,self.output_delta,timestep)  # Get the new momentum and the update function for bias \n",
        "                    self.layers[i].weights +=alpha*ADAM # Update the weights, special because \n",
        "                                                                    #it's the end of the backwardpass journey\n",
        "                    self.layers[i].bias += alpha*ADAM_bias # Update the bias \n",
        "\n",
        "                else : \n",
        "                    m[0],m[1],ADAM= self.adam_op(m,self.layers[i-1].output.T.dot(self.layers[i+1].delta),timestep)# Get the new momentum and the update function for weights\n",
        "                    m_b[0],m_b[1],ADAM_bias= self.adam_op(m_b,self.layers[i+1].delta,timestep)  # Get the new momentum and the update function for bias \n",
        "\n",
        "\n",
        "                    self.layers[i].weights += alpha*ADAM  # Update the weights\n",
        "                    self.layers[i].bias += alpha*ADAM_bias # Update the bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self,y,X,epoch=5, optimizer='SGD',history='False',alpha=0.01):  # Okay so , we can train now \n",
        "        if optimizer =='SGD':\n",
        "            for _ in range(epoch): # Number of epochs or how many time the entire train set is being used for minimizing the error .\n",
        "                for idx,el in enumerate(X): \n",
        "                        el=el.reshape(1,X.shape[1]) # Reshape, i don't want shape errors .\n",
        "                        self.forward(el) # One pass forward\n",
        "                        self.Backward(y[idx],el,alpha=alpha) # One pass backward   \n",
        "        if optimizer=='ADAM':  \n",
        "            for _ in range(epoch): # Number of epochs or how many time the entire train set is being used for minimizing the error . \n",
        "                self.backward_count += 1  # increment the number of backward pass done .\n",
        "                for idx,el in enumerate(X): \n",
        "                    \n",
        "                    el=el.reshape(1,X.shape[1]) # Reshape, i don't want shape errors .\n",
        "                    self.forward(el) # One pass forward \n",
        "                    self.Backward(y[idx],el,alpha=alpha,optimizer='Adam')  #One pass backward but with adam_optimizer\n",
        "                \n",
        "\n",
        "                \n",
        "                    \n",
        "\n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGPa6FLxWzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = NeuralNet(5) # Input_size = 5\n",
        "test.add(DenseLayer(15,activation='Lecun_tanh')) # I want to try a Lecun activation Function ! \n",
        "test.add(DenseLayer(10,activation='Lecun_tanh'))# Lecun_tanh is great \n",
        "test.add(DenseLayer(1,activation='Lecun_tanh',out=True))\n",
        "\n",
        "\n",
        "test2 = NeuralNet(5) # Input_size = 5\n",
        "test2.add(DenseLayer(15,activation='Lecun_tanh')) # I want to try a Lecun activation Function ! \n",
        "test2.add(DenseLayer(10,activation='Lecun_tanh'))# Lecun_tanh is great \n",
        "test2.add(DenseLayer(1,activation='Lecun_tanh',out=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0DKSz8vnbnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_classification # for generating a data set \n",
        "from sklearn.model_selection import train_test_split # For getting validation data and shuffling the data too\n",
        "X, Y = make_classification(n_samples=1000 ,n_features=5, n_redundant=0, n_informative=2,\n",
        "                             n_classes=2)  # we get X with features and Y with label\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,shuffle=True,test_size=0.3) # Splitting for train and test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ZgFBnsE96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test.train(y_train,x_train,epoch=5,optimizer='ADAM')  # Let's train our data with ADAM  with 5 epochs !! \n",
        "test2.train(y_train,x_train,epoch=5,optimizer='SGD') # Let's train our data with SGD for compare  with 5 epochs !! \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H-fc7LBYFYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred_to_binary(x): \n",
        "    for ind,val in enumerate(x): \n",
        "        if val >= 0.5:\n",
        "            x[ind]= 1 \n",
        "        else : \n",
        "            x[ind]=0\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Q4QmE8E-Zk",
        "colab_type": "code",
        "outputId": "3531dcfe-a61f-47a1-8f4d-e9d26422158b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "\n",
        "testy=[] \n",
        "for i in range(len(x_test)):\n",
        "    prediction= test.forward(x_test[i],return_=True)\n",
        "    testy.append(prediction) \n",
        "testy=pred_to_binary(testy)\n",
        "from sklearn.metrics import classification_report  \n",
        "\n",
        "\n",
        "testy2=[] \n",
        "for i in range(len(x_test)):\n",
        "    prediction= test2.forward(x_test[i],return_=True)\n",
        "    testy2.append(prediction) \n",
        "testy2=pred_to_binary(testy2)\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "\n",
        "print(\"===========================================\")\n",
        "print(\"With Adam optimization\")\n",
        "print(\"===========================================\")\n",
        "print(classification_report(y_test,np.round(testy).reshape(300)))   \n",
        "print(\"===========================================\")\n",
        "print(\"With Stochastic Gradient Descent optimization\")\n",
        "print(\"===========================================\")\n",
        "print(classification_report(y_test, np.round(testy2).reshape(300)))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================================\n",
            "With Adam optimization\n",
            "===========================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       144\n",
            "           1       0.93      0.96      0.95       156\n",
            "\n",
            "    accuracy                           0.94       300\n",
            "   macro avg       0.94      0.94      0.94       300\n",
            "weighted avg       0.94      0.94      0.94       300\n",
            "\n",
            "===========================================\n",
            "With Stochastic Gradient Descent optimization\n",
            "===========================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91       144\n",
            "           1       0.93      0.88      0.91       156\n",
            "\n",
            "    accuracy                           0.91       300\n",
            "   macro avg       0.91      0.91      0.91       300\n",
            "weighted avg       0.91      0.91      0.91       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxOTd9x35sX8",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion on Adam Optimizer \n",
        "You can see that on the same amount of epochs (5), the adam optimizer converge faster to the good update of the weights. For a big amount of epochs the SGD and the ADAM optimizer will have nearly the same efficiency. But on a big amount of data , with complex structure, time is important, and with Adam you are faster on optimization ! "
      ]
    }
  ]
}